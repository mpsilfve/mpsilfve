\chapter{Morphology and Morphological Tagging}

\section{Morphology}
%\begin{itemize}
%\item The morphological system.
%\item Word, morpheme, lemma, stem.
%\item Word classes.
%\item Inflectional categories.
%\end{itemize}

Words are the most readily accepted linguistic units at least in
written language, especially Western written language. They are
segments of letters and possible numbers which are surrounded by
white-space or punctuation. Although matters are more complex in
spoken language, written languages that do not use white space (such
as Chinese) and sign language, language users, even small children,
often have an intuitive idea about what constitutes a word in their
native language \cite{someone}.

\paragraph{Morphemes} Morphology is a sub-field of linguistics which
studies words. According to \cite{Bybee85}, morphology has
traditionally been concerned with charting the {\it morpheme
  inventory} of language. That is, finding the minimal semantic units
of language and grouping them into classes according to behaviour and
meaning. For example, the English word form ``dogs'' consists of two
morphemes ``dog'' and ``s''. The first one being a {\it word stem}
and the second one being an {\it inflectional affix} marking plural
number.

\paragraph{Non-Concatenative Morphology} In many languages, such as
English, words are mainly constructed from pieces such as ``dog'',
``cat'' and ``-s''. This is, however, not always true. For example,
English plural number can be signaled by other less transparent means
as demonstrated by the word pairs ``mouse/mice'' and
``man/men''. Morphological phenomena such as inflection and derivation
which fall beyond the scope of simple concatenation of morphemes into
words are called {\it non-concatenative}. 

The most common form of non-concatenative morphology is {\it
  suppletion}. Suppletion is the irregular relationship between word
forms such as the English singular ``ox'' and plural ``oxen''. Such
irregularity occurs in all languages. The example of ``mouse'' and
``mice'' can be analyzed as an instance of {\it ablaut} which is
another form of non-concatenative morphology where a morphological
process is signaled by a vowel change in the stem (/ou/ $\rightarrow$
/i/ in this case). 

Probably all languages which use concatenative morphology also have
{\it morphophonological alternations}. These are sound changes that
occur when morphemes are joined together. A cross-linguistically
common example is nasal assimilation \citep[p. 29]{Carr1993}, where
the place of articulation of a nasal depends on the following
stop. For example, the ``n'' in the prefix ``in'' in ``input'' and
``inset'' is pronounced as ``m'' and ``n'',
respectively. Morphophonological alternations can be seen as an
instance of non-concatenative morphology.

Languages differ with regard to the amount of non-concatenative
morphology. Some, like Turkish employ almost exclusively concatenation
to form complete word forms from morphemes. Others, such as English
employ a mix of concatenation and non-concatenative phenomena. Still,
concatenation is probably found to some degree in all languages
\cite{someone}. Especially in languages with rich morphology, such as
Finnish or Turkish, concatenation is the most important way to form
words. From the point of view of language technology, it is therefore
of great importance to be able to handle concatenative morphology.

\paragraph{Morphotax} Stems in English can often occur on their own as
words and are therefore called {\it free morphemes}. Inflectional
affixes cannot. Therefore, they are called {\it bound morphemes}. More
generally, morphology is concerned with rules governing acceptable
combinations of morphemes. For example, ``dog'' and ``dogs'' are valid
English word whereas ``dogdog'' and ``s'' in the meaning of plural
number are not. The rules governing the combination of morphemes into
words is called morphotax.

\paragraph{Word Class} The word forms ``dogs'' and ``cats'' share a
common number marker ``s''. However, their stems differ. Still, there
is a relation between the stems ``dog'' and ``cat'' because they can
occur with similar inflectional affixes. Therefore, they can be
grouped into a common {\it word class}: nouns. One can also say that
the {\it part-of-speech} of ``dog'' and ``cat'' is noun.  The
inventory of word classes in a language cannot be determined solely
based on word internal examination. Instead one has to combine
knowledge about the structure of words with knowledge about
interaction of the words in sentences. The concept of word class,
therefore, resides somewhere between the linguistic disciplines
morphology and {\it syntax} which is the study of combination of words
into larger units, phrases and sentences.

\paragraph{Lexeme and Lemma} Word forms such as ``dog'' ``dogs'' and
``dog's'' share a common stem ``dog''. Each of the word forms refers
to the concept {\sc dog}, however different forms of the word are
required depending on context. Different word forms, that denote the
same concept, belong to the same {\it lexeme}. Each lexeme has a {\it
  lemma} which is a designated word form representing the entire
lexeme. In the case of English nouns, the lemma is simply the singular
form, for example ``dog''. In the case of English verbs, the
infinitive, for example ``to run'', is usually used. The particular
choice depends on linguistic tradition. 

Lemmas are important for language technology because dictionaries
usually contain lemmas. Therefore, it is useful to be able to {\it
  lemmatize} a word form, that is produce the lemma given a word form.

\paragraph{Categories of Bound Morphemes} Whereas free morphemes are
grouped into word classes, bound morphemes are grouped into their own
categories according to meaning and co-occurrence restrictions. For
example, Finnish nouns can take a singular number
marker. Additionally, they can take one case marker from an inventory
of $15$ possible case markers, one possessive suffix from an inventory
of $6$ possible markers and a number of clitic affixes
\citep{Hakulinen2004}. The categories of bound morphemes usually
belong to particular word classes, however, several word classes may
share a particular class of bound morphemes. For example, both
adjectives and nouns take a number in English.

\paragraph{Morphological analysis} In many applications such as
information retrievel systems and syntactic parsers, it is useful to
be able to provide an exhaustive decription of the morphological
information associated with a word form. Such an descrption is called
a {\it morphological analysis} or {\it morphological tag} of the word
form. For example, the English word form ``dogs'' could have a
morphological analysis ``dog+Noun+Plural''. The granularity and
appearance of the morphological analysis depends on linguistic
tradition and the linguistic theory which is being applied, however
the key elements are the lemma of the word form as well as a list of
the bound morphemes associated to the word form.

\section{Morphologically Complex Languages}
\begin{itemize}
\item Typological classification of languages.
\item ``Large label sets''.
\end{itemize}

\section{Morphological Analyzers}
Word forms in natural languages can be ambiguous. For example, the
English ``dogs'' is both the plural form of noun and the present third
person form of a verb. The degree of ambiguity varies between
languages. It is also, to some degree, a function of the morphological
descption. Namely, coarse morphological desciptions can give less
ambiguity than finer ones. A morphological analyzer is a system which
processes word forms and returns the complete set of possible
morphological analyses for each word form.

\paragraph{Applications} Morphological analyzers are useful both when
the lemma of the word is important and when the information about
bound morphemes is required. The lemma is useful in tasks where the
semantics of the word form is of great importance. These task include
information extraction \cite{someone} and topic modeling
\cite{someone}. Information about bound morphemes is more important
for syntactic parsing and chunking which aim at uncovering the
structure of linguistic utterances.

\paragraph{Motivation} The need for full scale morphological analyzers
can be contested. For example \cite{somone} has argued that practical
applications can mostly ignore morphology and focus on processing raw
word forms instead of morphologically analyzed words. This may be a
valid approach for English and other languages which mainly utilize
syntactic means like word order to express grammatical information. In
these languages the number of word forms in relation to lexemes tends
to be low. For example, in the Penn Treebank of English
\cite{Marcus1993} spanning approximately 1 million words, three
distinct word forms occur which have the lemma ``dog'', namely
``dog'', ``dogs'' and ``dogged''. It can be argued that no specific
processing is required to proces English word forms.

In contrast to English, many languages do utilize morphology
extensively. For example, although the Finnish FinnTreeBank corpus
\cite{someone} only spans approximately 160,000 words (this is less
than 20\% of the Penn Treebank), there are 14 distinct word forms
which have the lemma ``koira'' (the Finnish translation of ``a
dog'').\footnote{If different compound words of ``koira'', such as
  ``saksanpaimenkoira'' (German Shepard) are considered, there are 23
  forms of koira in the FinnTreeBank corpus.}  In total, the Penn
Treebank contains some 49,000 distinct word forms whereas the
FinnTreeBank contains about 46,000 word forms even though it is only
20\% of the size of the English corpus. These considerations
illustrate the need for morphological processing for languages like
Finnish which make extensive use of inflective morphology. Methods
which only process word forms simply will suffer too badly from data
parsity.

\paragraph{Types} There are different kinds of morphological analysis
systems. The first systems used for English information retreival were
stemmers, the most famous system being \cite{Porter1997} (originally
proposed in 1979). The {\it Porter stemmer} uses a collection of rules
which strip suffixes from word forms. For example, ``connect'',
``connection'' and ``connected'' would all receive the stem
``connect''. The system does not include a vocabulary and can thus be
applied to arbitrary English word forms. The Porter stemmer, and
stemmers in general, are sufficient for information retrieval in
English but the fall short when more elaborate morphological
information is required, for example, in parsing. Moreover, they are
too simplistic for morphologically complex languages like Finnish and
Turkish (although \cite{Kettunen2005} show that a more elaborate
stemmer which can give several stem candidates for a word form can
perform comparable to a full morphological analyzer).

Morphological segmentation software, such as Morfessor
\cite{Creutz2002}, are another type of morphological analyzers often
utilized in speech recognition for languages with rich morphology. The
Morfessor system splits word forms into a sequence of morpheme like
substrings. For example, the word form ``dogs'' could be split into
``dog'' and ``s''.  This type of morphological segmentation is useful
in a wide variety of language technological applications, however it
is more ambiguous than a traditional morphological analysis where the
bound morphemes are represented by linguistic analysis symbols such as
+plural. Moreover, Morfessor output does not contain information about
morphological categories that are not overtly marked. For example, in
Finnish the singular number of nouns is not overtly marked (only
plural number is marked by an affix ``i''). This information can
however be very useful for example in syntactic parsing.

The current state-of-the-art for morphological analysis of
morphologically complex languages are finite-state morphological
analyzers \citep{Kaplan1994,Koskenniemi1984}. Full scale finite-state
analyzers can return the full set of analyses for word forms. They can
model morphotax and morphophonological using finite-state rules and
lexicon \citep{Beesley2003}. In contrast to stemmers, which are quite
simple, and segmentation systems like Morfessor which can be trained
in an unsupervised manner, full-scale morphological analyzers
typically require a lot of manual work. The most labor intensive part
of the process is the accumulation of the lexicon. 

Although, full-scale morphological analyzers require a lot of manual
work, the information they produce is very reliable. Coverage is a
slight problem because lemmas typically need to be manually added to
the system before word forms of that lemma can be analyzed. However,
morphological guessers can constructed from morphological analyzers
\cite{Linden2009}. These extend the analyzer to previously unseen
words based on similar words that are known to the analyzer.

The morphological analyzer primarily employed in this thesis is for
Finnish Open-Source Morphology OMorFi \citep{Pirinen2011}. It is a
morphological analyzer of Finnish implemented using the open-source
finite-state toolkit HFST \cite{Linden2009hfst} and is utilized for
the experiments presented in Chapter \ref{chapter:finnpos}.
 
\section{Morphological Tagging and Disambiguation}
I define morphological tagging as the task of assigning each word in a
sentence a unique full morphological analysis consisting of a lemma
and a morphological label which specifies the part-of-speech of the
word form as well as the bound morphemes that occur in the word form.

Morphological disambiguation and part-of-speech tagging are
interesting tasks because
they represent labeling tasks where both the set of inputs and outputs
are unfathomably large. Since each word in a sentence $x = (x_1,\
...,\ x_T)$ of length $T$ receives one label, the complete sentence
has $n^T$ possible labels $y = (y_1,\ ...,\ y_T)$ when the POS label
set has size $n$. For a sentence of $40$ words and a label set of $50$
labels, the number of possible label sequence is $40^{50} \approx
10^{80}$ which according to Wolfram Alpha\footnote{{\tt{\url
    http://www.wolframalpha.com/input/?i=number+of+atoms+in+the+universe}}}
is the estimated number of atoms in the observable universe.

The exact number of potential English sentences of any given length,
say ten, is difficult to estimate because all strings of words are not
valid sentences\footnote{Moreover, it is not easy to say how many word
  types the English language includes.}. However, it is safe to say
that it is very large -- indeed much larger than the combined number
of sentences in POS annotated English language corpora humankind is
likely to ever produce. Direct estimation of the conditional
distributions $p(y\cond x)$, for POS label sequences $y$ and sentences
$x$, by counting is therefore impossible.

Because the POS labels of words in a sentence depend on each other,
predicting the label $y_t$ for each position $t$ separately is not an
optimal solution. Consider the sentence ``The police dog me constantly
although I haven't done anything wrong!''. 

The labels of the adjacent words ``police'', ``dog'', ``me'' and
``constantly'' help to disambiguate each other. A priori, we think
that ``dog'' is a noun since the verb ``dog'' is quite rare. This
hypothesis is supported by the preceding word ``police'' because
``police dog'' is an established noun--noun collocation. However, the
next word ``me'' can only be a pronoun, which brings this
interpretation into question. The fourth word ``constantly'' is an
adverb, which provides additional evidence against a noun
interpretation of ``dog''. In total, the evidence points toward a verb
interpretation for ``dog''.

The disambiguation of the POS label for ``dog'' utilizes both so
called {\it unstructured} and {\it structured} information. The
information that ``police dog'' is a frequent nominal compound is
unstructured information, because it refers only to the POS label (the
prediction) of the word ``dog''. The information that verbs are much
more likely to be followed by pronouns than nouns is a piece of
structured information because it refers to the combination of several
POS labels. Structured information refers to the combination of
predictions. Both kinds of information are very useful, but a model
which predicts the label $y_t$ for each position in isolation cannot
utilize structured information.

Even though structured information is useful, structure is probably
mostly useful in a limited way. For example the labels of ``dog'' and
``anything'' in the example are not especially helpful for
disambiguating each other. It is probably a sensible assumption that
the further apart two words are situated in the sentence, the less
likely it is that they can significantly aid in disambiguating each
other. However, this does not mean that the interpretations of words
that are far apart cannot depend on each other -- in fact they
frequently do. For example embedded clauses introduce long range
dependencies inside sentences.

It is said that machine learning is sophisticated counting of
co-occurrences. This statement applies extremely well to POS
tagging. Counting is an adequate approach to capturing correlations
between the labels of words inside a small window (in the league of
five words), because most adjacent words indeed do depend on each
other is some way. However, sophisticated counting fails for larger
windows, because the number of meaningful dependencies in large
windows is negligible in comparison to the space of possibilities.

\section{Research into Morphological Tagging}
\begin{itemize}
\item POS tagging vs. Morphological tagging.
\item Statistical vs. rule-based.
\item Is it a reasonable division?
\item Influential rule based approaches: \cite{Brill1992}
  \cite{Karlsson1995}.
\item First successful HMM statistical approaches: \cite{Church1988}
  \cite{DeRose1988}.
\item Problem: A generative tagger has to model the input
  sentences. This forces the use of simple local features.
\item Solutions: Either try to model the input using a more elaborate
  model which does not work \cite{Ruokolainen2013}.
\item Better local features using a discriminative
  approach\cite{Ratnaparkhi1997}.
\item Still a well devised HMM model can surpas \cite{Brants2000},
  \cite{Halacsy2007}.
\item \cite{Silfverberg2010} and \cite{Silfverberg2010} implement HMM
  taggers as finite-state systems.
\item Label bias \cite{Lafferty2001} and observation bias \cite{Klein2002}.
\item Solution: Conditional random fields \cite{Lafferty2001}.
\item Another Solution: Support Vector Machines \cite{Cortes1995},
  \cite{Gimenez2004}.
\item A simple solution: Averaged perceptron taggers \cite{Collins2002}.
\item These can be seen as alternative estimators for a discriminative
  HMM tagger.
\item \cite{Ruokolainen2014} examines several different approaches to
  parameter estimation.
\item Applications to morphological tagging: Marmot using a cascaded
  model \cite{Muller2013} and Morfette \cite{Chrupala2008}.
\item Joint Lemmatization and tagging \cite{Muller2015}.
\item Tagging can also be combined with morphological segmentation
  \cite{MullerX}.
\item \cite{Silfverberg2014} use sub-label features and
  \cite{Silfverberg2015} combine them with a cascaded approach.
\item \cite{Chrupala2008} and \cite{Silfverberg2015} adopt the same
  approach to lemmatization although the particular features of the
  lemmatizer differ.
\end{itemize}