\chapter{Experiments}
\label{chapter:finnpos}

Publication \ref{pub:6} presents FinnPos, which is a discriminative
morphological tagger based on the averaged perceptron classifier. It
is specifically geared toward morphologically rich languages. To this
end, it implements beam search and a cascaded model architecture for
speeding up estimation and tagging speed. To improve accuracy, it
includes an option to use a morphological analyzer during
tagging. Moreover, it extracts sub-label dependencies for unstructured
and structured features in order to improve tagging accuracy in presence of
large structured label sets.

This chapter presents experiments on morphological tagging in Finnish
using FinnPos. These experiments are intended to augment the
experiments in Publication \ref{pub:6}. I will present experiments on
the following themes

\begin{itemize}
\item {\bf Cascaded Model Architecture} What is the effect of different settings for the label guesser on tagging accuracy, estimation speed and tagging speed?
\item {\bf Beam Search} What is the effect of beam width on tagging accuracy, estimation speed and tagging speed?
\item {\bf Model Order} What is the effect of model order on tagging accuracy, estimation speed and tagging speed? Experiments are presented both when tagging with and without a morphological analyzer.
\item {\bf Sub-Label Order} What is the effect of the order of sub-label dependencies on tagging accuracy, estimation speed and tagging speed? Experiments are presented both when tagging with and without a morphological analyzer.
\item {\bf Model Pruning} Which is the better model pruning strategy: update count based or value based pruning?
\end{itemize}



\section{Data Sets}
All experiments are conducted on both the Turku Dependecy Treebank
\citep{Haverinen2013} (TDT) and FinnTreeBank \citep{Voutilainen2011}
(FTB). Two distinct data sets, containing text from different genres,
are used in order to justify general claims about the performance of
the system in Finnish morphological tagging. Table \ref{tab:data} gives a
numerical overview of both data sets.

\begin{table}[htb!]
%\begin{small}
\begin{center}
\begin{tabular}{lcc} 
 & TDT & FTB \\
\hline
size & 13,572 sent. (183,118 tok.) & 19,121 sent. (162,028 tok.) \\
\# labels & 2,014 & 1,399 \\
OMorFi coverage & 94.2\% & 99.0\% \\
\end{tabular}
\end{center}
%\end{small}
% \caption{A post-processed and disambiguated OMorFi analysis with Turku Dependency Treebank annotation for an exemplar sentence \emph{H\"an ei asu pieness\"a kyl\"ass\"a} adapted from \citet{haverinen2014}.}
\caption{Summary of Turku Dependency Treebank (TDT) \citep{Haverinen2013} and FinnTreeBank (FTB) \citep{Voutilainen2011}. The OMorFi coverage refers to coverage per token.}
\label{tab:data} 
\end{table}

\paragraph{FinnTreeBank}
FTB is a morphologically tagged and dependency parsed collection of
example sentences from Iso Suomen Kielioppi, a descriptive grammar of
the Finnish language \citep{Hakulinen2004}. The examples have been
harvested from newspapers, novels and blogs. Additionally, some
examples represent spoken language. 

Each sentence in the FTB corpus has been selected to illustrate some
grammatical phenomenon. Therefore, it is safe to say that FTB does not
represent a random sample of Finnish text. It probably contains a high
number of rare grammatical phenomena. This can be seen as a weakness
because results on the FTB corpus may not carry over to other data
sets. However, it also makes the corpus interesting and challenging
from the point of view of morphological tagging because the data is
expected to be complex.

Both the morphological tagging and dependency structures of FTB have
been manually prepared.  The morphological analyses of word tokens are
post-processed outputs of OMorFi, an open-source morphological
analyzer for Finnish
\citep{Pirinen2011}. % The post-processing steps reported by \citet[Section 5.2]{haverinen2014} comprise x, y, and z.

\paragraph{Turku Dependency Treebank} TDT contains text from ten different domains, for example Wikipedia articles, blog entries, and financial news. The annotation has been prepared by manually correcting the output of an automatic annotation process.
%The domains and the respective section sizes are presented in Table \ref{tab: TDT}. 
%In addition, the treebank contains a publicly non-available test set of 1,554 sentences (21,211 tokens) for evaluation purposes.
Similarly to FTB, the morphological analyses of word tokens in FTB are
post-processed outputs of OMorFi \citep{Pirinen2011}. However, the
treebanks are based on different versions of OMorFi. Moreover, the
post-processing steps applied in TDT and FTB differ. This results in
somewhat different annotation schemes.  The TDT annotation for each
word token consists of word lemma (base form), part-of-speech (POS),
and a list of detailed morphological information, including case,
number, tense, and so forth.

\paragraph{Data Splits} Widely used data sets usually have established
splits into training, development and test data. For example, most
work on English POS tagging reports results on the Penn Treebank
corpus \citep{Marcus1993} using the data splits introduced by
\cite{Collins2002} (Sections 1-18 for training, 19-21 for development
and 22-24 for testing). This is sound because it guarantees that
results reported in different papers are comparable. For the data sets
used in this thesis, FTB and TDT, there are no established
splits. Therefore, the experiments use 80\% of the data for training,
10\% as development data and 10\% as final test data. The data is
split in the following way: For each consecutive ten sentences, the
first eight are assigned to the training set, the ninth one to the
development set and the tenth one to the test set.

\section{Setup}

%This section is intended to augment the treatment
%in Publication \ref{pub:5}. It presents several experiments on
%morphological tagging using the FinnPos tagger toolkit which were
%omitted from the paper in favor of clarity.

Exhaustive experiments on the effect and interactions of the different
hyper-parameters used by the FinnPos tagger would require hundreds of
experiments because FinnPos incorporates a variety of optimizations to
accuracy and speed governed by different
hyper-parameters.\footnote{Experiments should vary beam width, the
  settings of the label guesser, model order and order of sub-label
  dependencies. Moreover, experiments should be conducted using a
  morphological analyzer and without one. Assuming 10 different
  settings for beam, and guesser and three settings for model order
  and sub-label order, this gives $10^2\cdot 3^2 \cdot 2 = 1800$
  distinct experiments.} I did not deem this feasible in
practice. Instead I have chosen the settings presented in Publication
\ref{pub:6} as vantage point and examined the impact of changing one
hyper-parameter at a time. These settings were chosen because they
give state-of-the-art accuracy as presented in the paper.

The basic setting for all experiments is 
\begin{itemize}
\item A second order model.
\item First order sub-label dependencies.
\item 99.9\% mass for the generative label guesser.
\item 99.9\% mass for the adaptive beam search.
\end{itemize}
This setting is varied with respect to the hyper-parameter that is
being investigated. All experiments are run on a desktop computer
(Intel Core i5-4300U with 1.90 GHz and 16 GB of memory).

In all experiments, hyper parameters are first set using development
data. Then the development data and training data are combined and
this data set is used to train the final model, which is then
evaluated using the test set. Training times refer to training the
final model. Thus they do not contain the cost of development. This
setup was used because it is also used in Publications \ref{pub:4},
\ref{pub:5} and \ref{pub:6}.

A morphological analyzer is used when specifically indicated. A label
dictionary is used in all experiments both to speed up decoding and
improve accuracy. The label dictionary is also used when a
morphological analyzer is used in tagging. The reason for this is that
a liberal compounding and derivation mechanisms (such as the ones
implemented in the OMorFi morphological analyzer) can result in a
number of unlikely analyses for longer word forms. Analyses that have
been attested in the training corpus should, therefore, be preferred
when possible.

The feature set used in the experiments follows Publication
\ref{pub:6}.  Let $x = (x_1 ... x_T)$ be a sentence, $y = (y_1
... y_T)$ a label sequence and $t$ and index. Then the unstructured
features templates for the morphological tagger are the familiar
Ratnaparkhi features \citep{Ratnaparkhi1998} augmented with a few
additional features. For all words, FinnPos uses the following feature
templates
\begin{itemize}
\item The word form $x_t$ and the lower cased version of $x_t$.
\item The length $|x_t|$ of word form $x_t$.
\item Each word form in a four word window around $t$: $x_{t-2}$, $x_{t-1}$, $x_{t+1}$ and $x_{t+2}$.
\item The word form bigrams $(x_{t-1}, x_t)$ and $(x_t,x_{t+1})$.
\end{itemize}
For rare words\footnote{A rare word is one that is not common. The
  list of common words is user defined but in these experiments I have
  defined common words to be words having frequency 10 or higher in
  the training corpus}, it additionally extracts the following
features
\begin{itemize}
\item {\tt DIGIT} when $x_t$ contains a digit.
\item {\tt UC} when $x_t$ contains an upper case letter.
\item Prefixes and suffixes of $x_t$ up to length 10.
\end{itemize}
When a morphological analyzer is used, each morphological label given
to word $x_t$ is also used as a feature template.

%Let $x = x_1 ... x_n$ be a word form of length $n$ and $y$ its
%label. Then the features for the lemmatizer are
%\begin{itemize}
%\item The lower cased variant of $x$.
%\item Prefixes of $x$ up to length 5 and suffixes up to length 7.
%\item The infixes $x_{n - 2}x_{n - 1}$, $x_{n - 3}x_{n - 2}$ and $x_{n - 4}x_{n - 3}$.
%\item The label $y$ and its main part-of-speech.
%\item {\tt DIGIT} when $x$ contains a digit.
%\item {\tt UC} when $x$ contains an upper case letter.
%\end{itemize}
%Additionally a combination of each feature template and the
%morphological label $y$ is used as a feature template. Naturally, the
%combination of $y$ with itself is omitted.

Some baseline runs are impossible to run. FinnPos uses a second order
model. With a label set size of 1,000, inference using the plain
Viterbi algorithm is prohibitively slow because the time complexity of
the algorithm is dependent on the third order of the label set size.
Therefore, it was not feasible to run experiments without label
pruning during training.

Finally, the results of the experiments presented here differ minutely
from the results presented in Publication \ref{pub:6} due to added
features (lower cased word form and word length) and a few bug fixes
that have improved results.

\section{Using a Cascaded Model}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{cccc}
\multicolumn{4}{c}{Adaptive Guess Count}\\
Guess Mass & Tagging Accuracy (\%) & Training time & Dec. Speed (KTok/s)\\
\hline
0.9        & 93.21 (OOV: 77.68) & 3 min, 3 epochs &   7          \\
0.99       & 93.11 (OOV: 77.14) & 3 min, 3 epochs & 7            \\
0.999      & 93.23 (OOV: 78.49) & 4 min, 4 epochs            & 8            \\
0.9999      & 93.41 (OOV: 78.55) & 2 min, 2 epochs            & 7            \\
           &                  &               &               \\
\multicolumn{4}{c}{Fixed Guess Count}\\
Guess Count & Tagging Accuracy (\%) & Training time (min) & Dec. Speed (KTok/s) \\
\hline
1        & 91.48 (OOV: 69.81)           & 1 min, 3 epochs            & 8            \\
10       & 93.23 (OOV: 77.56)           & 2 min, 2 epochs            & 7            \\
20       & 93.18 (OOV: 77.89)           & 3 min, 3 epochs            & 7            \\
30       & 93.22 (OOV: 77.62)           & 4 min, 3 epochs            & 6            \\
40       & 93.43 (OOV: 78.49)           & 4 min, 2 epochs            & 5            \\
\end{tabular}
\caption{Different label guesser settings for FinnTreeBank}
\end{center}
\end{table}


\begin{table}[htb!]
\begin{center}
\begin{tabular}{cccc}
\multicolumn{4}{c}{Adaptive Guess Count}\\
Guess Mass & Tagging Accuracy (\%) & Training time (min) & Dec. Speed (KTok/s)\\
\hline
0.9        & 92.69 (OOV: 74.10)           & 8 min, 8 epochs           & 5            \\
0.99       & 92.66 (OOV: 74.13)           & 9 min, 8 epochs           & 5            \\
0.999      & 92.76 (OOV: 74.65)           & 8 min, 7 epochs            & 5            \\
0.9999      & 92.75 (OOV: 74.30)           & 6 min, 5 epochs            & 5            \\
           &                  &               &               \\
\multicolumn{4}{c}{Fixed Guess Count}\\
Guess Count & Tagging Accuracy (\%) & Training time (min) & Dec. Speed (KTok/s) \\
\hline
1        & 89.85  (OOV: 61.33)          & 2 min, 5 epochs            & 6            \\
10       & 92.35  (OOV: 72.55)          & 5 min, 7 epochs            & 6            \\
20       & 92.61  (OOV: 73.88)          & 5 min, 4 epochs            & 6            \\
30       & 92.81  (OOV: 74.87)          & 12 min, 9 epochs          & 5            \\
40       & 92.91  (OOV: 75.35)          & 14 min, 9 epochs            & 5            \\
\end{tabular}
\caption{Different label guesser settings for Turku Dependency Treebank}\label{tab:prune-res}
\end{center}
\end{table}

This section presents experiments on using different settings for the
generative label guesser included as a pre-pruning step during
training as explained in Section \ref{sec:prec-tagger}.

\paragraph{Setup}For both TDT and FTB, I present results for using a fixed amount of
label guesses and for choosing a varying amount of guesses per word
based on probability mass. Because of the prohibitive runtime and
memory requirements, it was not possible to run experiments without
any form of label pruning during training. The most important point of
these experiments is that using some kind of label guesser during
training is almost necessary if one wants to train a second order
model for label sets of several hundreds or thousands of labels.

\paragraph{Results} As Table \ref{tab:prune-res} demonstrates, using a
larger amount of label guesses improves accuracy in general. When
using a fixed number of guesses for every word, the accuracy levels
off at 40 guesses per word for both FTB and TDT.

When pruning label candidates based on probability mass, the results
differ between the treebanks. For FTB, the mass 0.9999 yields
approximately the same accuracy than as 40 guesses. For TDT, however,
40 label guesses result in $0.2\%$-points better accuracy than using
the mass $0.9999$. The difference is substantial.

The training time per epoch is clearly related to the amount of label
guesses, however, the number of epochs seems to fluctuate somewhat
from two to four for FTB and from five to eleven for TDT. Therefore,
it is difficult to see a clear trend for the total training time. It
is also not possible to say that mass based pruning always leas to a
faster training time when compared to a fixed number of guesses which
yield similar accuracy.

The amount of label guesses influences decoding speed to some degree
because the same setting is used for the label guesser during
decoding. Because the label guesser is only used for OOV words, the
exact setting of the guesser does however only have a moderate impact.

\paragraph{Discussion} Whereas training a second order tagger for a
label set exceeding a thousand labels requires a prohibitive amount of
computational resources when estimation and inference utilize the
standard Viterbi algorithm or even a beam search, the experiments in
this section demonstrate that a cascaded model architecture allows for
training second order models rather fast. The memory requirement was
moderate as it did not exceed the 16 GB available on the author's
computer.

It is not clear whether pruning based on probability mass is superior
to pruning based on a fixed amount of guesses. It is concerning that
the results on the TDT data set for the mass $0.9999$ were clearly
inferior to the result when using $40$ guesses for each
word. Increasing the the probability mass also did not seem to improve
the results further. A reason for this may be that the TDT corpus is
larger than the FTB corpus. The generative guesser may suffer from
over-confident probability estimates when the amount of training data
is increased and it may be quite difficult to set the threshold for
the probability mass. A discriminative guesser could be used instead,
but its training time would exceed that of the generative
guesser. This would reduce overall performance. An alternative might
be to decreases the amount of training data for the guesser, but this
requires further experiments.

\section{Beam Search}
This subsection presents experiments using different beam settings
during training and decoding. Experiments are presented both for fixed
beam widths and adaptive beam, which is described in Section
\ref{sec:prec-tagger}.

\paragraph{Setup} I compare fixed beam width to an adaptive beam
presented in Section \ref{sec:prec-tagger}. Additionally, I include
training and decoding results when no beam is used. The same beam
width is used during in training and when tagging the test set.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{cccc}
\multicolumn{4}{c}{Adaptive Beam}\\
Beam Width & Tagging Accuracy (\%) & Training time (min) & Dec. Speed (KTok/s)\\
\hline
0.9        & 93.08 (OOV: 76.99)           & 3 min, 3 epochs & 6           \\
0.99       & 93.14 (OOV: 77.44)           & 3 min, 3 epochs & 8            \\
0.999      & 93.28 (OOV: 80.49)           & 2 min, 2 epochs & 8            \\
           &                  &               &               \\
\multicolumn{4}{c}{Fixed Beam}\\
Beam Width & Tagging Accuracy (\%) & Training time & Dec. Speed (KTok/s) \\
\hline
1        & 92.32 (OOV: 75.07)            & 2 min, 2 epochs & 7          \\
10       & 93.33 (OOV: 78.28)           & 2 min, 2 epochs &  7            \\
20       & 93.11 (OOV: 77.29)           & 4 min, 3 epochs &  7             \\
$\infty$ & 93.31 (OOV: 78.19)           & 20 min, 2 epochs            & 6            \\
\end{tabular}
\caption{Different beam settings for FinnTreeBank.}
\end{center}
\end{table}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{cccc}
\multicolumn{4}{c}{Adaptive Beam}\\
Beam Width & Tagging Accuracy (\%) & Training time (min) & Dec. Speed (KTok/s)\\
\hline
0.9       & 92.55 (OOV: 73.73)           & 5 min, 5 epochs           & 6            \\
0.99       & 92.87 (OOV: 74.88)           & 7 min, 7 epochs            & 6            \\
0.999       & 92.76 (OOV: 74.65)            & 8 min, 7 epochs            & 6            \\
           &                  &               &               \\
\multicolumn{4}{c}{Fixed Beam}\\
Beam Width & Tagging Accuracy (\%) & Training time (min) & Dec. Speed (KTok/s) \\
\hline
1        & 91.80 (OOV: 71.26)           & 6 min, 9 epochs            & 6            \\
10       & 92.83 (OOV: 74.85)           &  7 min, 6 epochs           & 6            \\
20       & 92.60 (OOV: 73.98)           & 10 min, 7 epochs            & 6            \\
$\infty$       & 91.58  (OOV: 69.46)          & 199 min, 8 epochs            & 6            \\
\end{tabular}
\caption{Different beam settings for Turku Dependency Treebank.}
\end{center}
\end{table}


\paragraph{Results} It is difficult to see a clear relation between
the beam width and tagging accuracy. The fixed beam of width one is
clearly the worst for both TDT and FTB. Larger beams give improved
results when compared to beam width one, however, all larger beams seem
to give results in the same range. Moreover, increasing the beam from
10 to 20 results in a $0.1\%$-point drop in accuracy for
FTB. Additionally, the system without beam search performs worse
than systems with adaptive or fixed beam width for TDT.

A small beam width results in a faster training time than a larger
beam. When using an infinite beam, the training time for TDT is
surprisingly large. The reason for this is that there are sequences of
words in the training and development data which receive a large
number of label guesses. When no beam is used, this results in very
long tagging times for the sequences because a second order model is
used.

\paragraph{Discussion} It seems that the effect of beam width on
tagging accuracy is not easy to analyze. Beam width one gives inferior
results when compared to other beam settings but all other beam
settings seem to deliver accuracy in the same range. The effect on
training time is, however, clear. Larger beam width slows down
training. 

The effect of large beam widths on training time can be dramatic as
exemplified by the results for TDT using infinite beam width. Analysis
of the problem revealed that there are sentences in the treebank that
contain words which consist of characters that only occur in those
words. Such words will receive a very large amount of guesses when
label guesses are pruned based probability mass. This happens because
the suffix based label guesser uses Laplace smoothing. The
distribution $p(y|x)$ of labels $y$ given a word form $x$ becomes
almost flat when the suffixes of $x$ only occur a single time in the
training data. For example, the three Greek words in ``Larnakan
lentoasema (kreik. Διεθνές Aεροδρόμιο Λάρνακας) on Kyproksen
kansainvälinen lentoasema'' receive 1287 label guesses each, that is
every label in the TDT label set, when using mass $0.999$. When no
beam is employed, sequences of consecutive words with extremely many
labels candidates result in extreme tagging times for the
sentence.\footnote{FinnPos has now been fixed to employ a user defined
  ceiling on the amount of guesses used during training. This setting
  will, however, degrade the accuracy of the tagger to some
  extent. Therefore, the setting has not been used in these
  experiments.}

In contrast to training time, the effect of beam width on decoding
time is minimal. This happens because decoding uses the label
dictionary which means that for most words the tagger will choose the
label from a very restricted set of candidates, typically one or two
analyses.

Contrary to what the literature indicates
\citep{Huang2012,Collins2004}, violation fixing gave no significant
improvements in accuracy in preliminary experiments. As it only slows
down training, it was not included in FinnPos.

In conclusion, it seems that the effect of beam width on tagging
accuracy is erratic but the effect on training time is
clear. Therefore, it is probably recommendable to use a beam. The mass
based beam resulted in similar training times and tagging accuracies
as the fixed beam. 

I think it is interesting that infinite beam width results in inferior
results for TDT when compared with other beam settings. This effect
was also noted by \cite{Huang2012} when performing experiments in POS
tagging. I have no explanation for this at the current time.

\section{Model Order}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{cccc}
\multicolumn{4}{c}{Without a Morphological Analyzer}\\
Model Order & Tagging Accuracy (\%) & Training time & Dec. Speed (KTok/s)\\
\hline
0        & 91.91            & 3 min, 3 epochs            & 8            \\
1        & 92.49            & 3 min, 4 epochs            & 8            \\
2        & 92.49            & 4 min, 5 epochs            & 7            \\
                &                       &                  &            \\
\multicolumn{4}{c}{Using a Morphological Analyzer}\\
Model Order & Tagging Accuracy (\%) & Training time & Dec. Speed (KTok/s)\\
\hline
0        & 95.35            & 5 min, 9 epochs            & 25            \\
1        & 95.98            & 5 min, 10 epochs           & 23            \\
2        & 95.96            & 5 min, 8 epochs            & 24            \\
\end{tabular}
\caption{Different Model Orders for FinnTreeBank}
\end{center}
\end{table}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{cccc}
\multicolumn{4}{c}{Without a Morphological Analyzer}\\
Model Order & Tagging Accuracy (\%) & Training time (min) & Dec. Speed (KTok/s)\\
\hline
0        & 91.15            & 6 min, 4 epochs            & 6            \\
1        & 91.17            & 8 min, 8 epochs            & 5            \\
2        & 91.83            & 5 min, 3 epochs            & 5            \\
                &                       &                  &            \\
\multicolumn{4}{c}{Using a Morphological Analyzer}\\
\hline
0        & 95.53            & 5 min, 4 epochs            & 21            \\
1        & 96.05            & 5 min, 7 epochs            & 22            \\
2        & 96.13            & 5 min, 5 epochs            & 20            \\

\end{tabular}
\caption{Different Model Orders for Turku Dependency Treebank}
\end{center}
\end{table}

In this section I examine the impact of model order on tagging
accuracy, training time and decoding time. I examine the effect of
model order both when tagging without an analyzer and wen using an
analyzer.

\paragraph{Setup} The experiments in this section do not use sub-label
features in order to clearly reveal the impact of model order in
isolation of other factors.

\paragraph{Results} The accuracy on both FTB and TDT increases when
going from order zero to a first order model. Further increasing
model order only gives an improvement for TDT.

The increase in accuracy when going from a unstructured (order zero)
model to a second order model is approximately 0.5\%-points for both
FTB and TDT. This applies both when using a morphological analyzer and
when not using it. It is noteworthy that the increase in accuracy
resulting from the morphological analyzer is substantially larger for
both data sets.

\paragraph{Discussion} The fact that a second order model improves
results only for TDT could be a result of the fact that FTB is
smaller, however, the difference in corpus size is quite small (only about
11\% or about 20,000 words). A more likely explanation relates to the
fact that the average sentence length in TDT is 13 words, whereas
sentences in FTB only have 8 words on average. Higher sentence length
translates to longer average distance between syntactically
dependent words. Therefore, a second order model, which can model longer
dependencies, may be more helpful when tagging TDT.

Overall, it seems like the improvement from using a second order model
over a zeroth order model is quite small. Partly, this is probably a
result of the fact that both of the data sets are quite
small. Moreover, the unstructured word context features included in
the feature set partly overlap with structured features and thus
diminish their effect.

Finally, it is interesting to see that the improvement resulting from
model order is about equal when using a morphological analyzer and
when not using one even though the tagging accuracies for the zeroth
order models when using an analyzer and without one are about 4
\%-points apart. This shows that using the analyzer does not nullify
the improvement from other improvements to tagging accuracy. Ultimately,
the impact of the analyzer on tagging accuracy is however of a
higher magnitude than the impact of model order.
 
\section{Sub-Label Dependencies}

In this section, I examine the impact of sub-label order on
tagging accuracy, training time and decoding time both using a
morphological analyzer and without a morphological analyzer. The
results in this section differ slightly from Publication \ref{pub:6}
because of minor bug fixes and improvements to the feature set of the
tagger.

\paragraph{Setup} The different model configurations, which are
investigated and shown in Tables \ref{tab:ftb-sl-res} and
\ref{tab:tdt-sl-res}, are (1) no sub-label dependencies, (2)
unstructured sub-label dependencies, (3) unstructured and first order
sub-label dependencies and (4) unstructured, first and second order
sub-label dependencies.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{cccc}
\multicolumn{4}{c}{Without a Morphological Analyzer}\\
Sub-Label Order & Tagging Accuracy (\%) & Training time    & Dec. Speed (KTok/s)\\
\hline
None            & 92.49 (OOV: 74.68)    & 3 min, 5 epochs  & 6                       \\
0               & 93.05 (OOV: 77.74)    & 1 min, 2 epochs  & 5                       \\
1               & 93.29 (OOV: 78.40)    & 1 min, 2 epochs  & 4                       \\
2               & 92.68 (OOV: 75.22)    & 5 min, 4 epochs  & 6                       \\
                &                       &                  &                          \\
\multicolumn{4}{c}{Using a Morphological Analyzer}\\
Sub-Label Order & Tagging Accuracy (\%) & Training time    & Dec. Speed (KTok/s)\\
\hline
None     & 95.98 (OOV: 91.41)           & 3 min, 8 epochs  & 25                       \\
0        & 96.08 (OOV: 91.98)           & 1 min, 3 epochs  & 22                       \\
1        & 96.24 (OOV: 92.28)           & 1 min, 2 epochs  & 21                       \\
2        & 96.31 (OOV: 92.58)           & 4 min, 3 epochs  & 19                       \\
\end{tabular}
\caption{Different Sub-Label Orders for FinnTreeBank}\label{tab:ftb-sl-res}
\end{center}
\end{table}


\begin{table}[htb!]
\begin{center}
\begin{tabular}{cccc}
\multicolumn{4}{c}{Without a Morphological Analyzer}\\
Sub-Label Order & Tagging Accuracy (\%) & Training time    & Dec. Speed (KTok/s)\\
\hline
None            & 91.89 (OOV: 70.63)    & 2 min, 3 epochs  & 5                       \\
0               &  92.59 (OOV: 73.98)   & 2 min, 4 epochs  & 5                       \\
1               &  92.69 (OOV: 74.35)   & 5 min, 7 epochs  & 3                       \\
2               &  92.31 (OOV: 72.31)   & 13 min, 8 epochs  & 5                       \\
                &                       &                  &                          \\
\multicolumn{4}{c}{Using a Morphological Analyzer}\\
Sub-Label Order & Tagging Accuracy (\%) & Training time    & Dec. Speed (KTok/s)\\
\hline
None            &  96.12 (OOV: 91.12)   & 3 min, 5 epochs  & 19                       \\
0               &  96.17 (OOV: 91.39)   & 2 min, 5 epochs  & 18                       \\
1               &  96.39 (OOV: 91.84)   & 3 min, 5 epochs  & 16                       \\
2               &  96.29 (OOV: 91.69)   & 12 min, 8 epochs  & 16                       \\
\end{tabular}
\caption{Different Sub-Label Orders for Turku Dependency Treebank}\label{tab:tdt-sl-res}
\end{center}
\end{table}


\paragraph{Results} When a morphological analyzer is not used as part
of the tagger, total improvement in accuracy stemming from sub-label
dependencies is approximately 0.8\%-points for both FTB and TDT. This
is higher than the improvement stemming from increasing model order
(from model order zero to model order two).

When a morphological analyzer is used as part of the tagger, total
improvement in accuracy provided by sub-label dependencies is
approximately the same as transitioning from model order zero to one.

Only for FTB, do second order sub-label features give added accuracy
compared to first order features and only when using the morphological
analyzer. However, the improvement is not statistically
significant. In other cases, second order sub-label dependencies
degrade performance compared to first order sub-label dependencies.

As can be expected, training time increases and decoding speed
decreases with increasing sub-label order. However, sub-label
dependencies seems to decrease the amount of training epochs needed to
converge to the final model parameters.

When a morphological analyzer is not used, unstructured sub-label
features are more influential for accuracy than structured sub-label
features for both FTB and TDT.  When the analyzer is used, the
opposite is true. Structured sub-label dependencies improve
performance substantially more. In fact, unstructured sub-label
dependencies alone do not provide a statistically significant
improvement, when the analyzer is used, whereas a combination of
unstructured and first order structured sub-label dependencies
do. Moreover, the improvement given by first order sub-label
dependencies with regard to unstructured dependencies is greater when
the analyzer is used than when it is not used.

\paragraph{Discussion} As stated in Section \ref{sec:prec-tagger}, sub-label dependencies can improve accuracy in two ways
\begin{enumerate}
\item they can counteract data sparsity and
\item capture congruence and other phenomena that transcend individual
  word classes.
\end{enumerate}
Experimentally, it is difficult to discern these two effects (probably
the experiments in this section cannot do this). Perhaps they are not
even distinct effects. After all, in the presence of a sufficient
amount of training data, all combinations of full labels are observed
and there is no need for modeling congruence and other similar
phenomena using sub-label dependencies. In practice the data is,
however, always sparse. And I think that in the case of insufficient
data, a stronger structured model can yield better results because it
utilizes the training data in a richer manner.

Unstructured sub-label dependencies probably mainly act to reduce data
sparsity. This is a credible explanation for the improvement in
accuracy because their effect is almost completely nullified by the
morphological analyzer whose main purpose is similarly to counteract data
sparsity which arises because of the large variety of inflections in
Finnish text. In contrast, the effect of structured sub-label
dependencies is not nullified by the morphological analyzer. In fact
the improvement is greater when the analyzer is used, when compared to
the setting where only unstructured sub-label dependencies are used. 

It is possible that the effect of structured sub-label dependencies
still mainly stems from reduced data sparsity in the structured model
but a part of the reduction of data sparsity is that significant
grammatical relations can be learned from the data.

The experiments show that sub-label dependencies deliver at least as
great improvements as increasing model order which is a standard trick
in sequence labeling. Still, however, the impact from using a
morphological analyzer dwarfs both of these effects.

\section{Model Pruning}

In this section, I examine two model pruning strategies: pruning by
update count and pruning by parameter mass. The strategies are
presented in \ref{sec:pruning}.

\paragraph{Setup} The value for the pruning parameter was set using
development data. The range of parameter values was chosen so as to
show the difference in pruning efficiency. The specific parameter
values are not very important. The important aspect is the relation of
model size and accuracy. These experiments only investigate pruning
for tagger parameters. Pruning could, however, also be applied to the
data-driven lemmatizer.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{c|ccccc}
\multicolumn{1}{c}{}        & \multicolumn{5}{c}{Update Count Threshold}\\
MA                     & None           & $< 2$        & $< 3$        & $< 4$        & $< 5$       \\
\hline
no                     & 93.2\%, 4.8M   & 93.2\%, 3.9M & 93.2\%, 3.6M & 93.2\%, 3.0M & 93.1\%, 1.0M \\
yes                    & 96.3\%, 4.3M   & 96.2\%, 3.9M & 96.2\%, 3.7M & 96.2\%, 3.3M & 96.1\%, 1.0M \\
                       &                &              &              &              &              \\
\multicolumn{1}{c}{}                        & \multicolumn{5}{c}{Parameter Mass Threshold}\\
MA                     & None         & $< 2.0$      & $< 2.5$      & $< 3$      & $< 3.5$        \\
\hline
no                     & 93.2\%, 4.8M & 93.3\%, 1.8M & 93.2\%, 1.4M & 93.2\%, 1.2M & 93.2\%, 0.9M \\
yes                    & 96.3\%, 4.3M & 96.3\%, 0.9M & 96.3\%, 0.7M & 96.2\%, 0.3M & 96.1\%, 0.1M \\
\end{tabular}
\caption{Result of applying different pruning strategies on FinnTreeBank models.}\label{tab:pruning-ftb}
\end{center}
\end{table}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{c|ccccc}
\multicolumn{1}{c}{}                       & \multicolumn{5}{c}{Update Count Threshold}\\
MA                     & None            & $< 2$        & $< 3$        & $< 4$        & $< 5$       \\
\hline
no                     & 92.8\%, 6.4M    & 92.7\%, 5.2M & 92.7\%, 5.0M & 92.8\%, 4.9M & 92.6\%, 4.9M \\
yes                    & 96.3\%, 5.5M    & 96.4\%, 5.0M & 96.3\%, 4.9M & 96.4\%, 4.9M & 96.3\%, 4.9M \\
                       &                 &              &              &              &              \\
\multicolumn{1}{c}{}     & \multicolumn{5}{c}{Parameter Mass Threshold}\\
MA                     & None           & $< 4.0$        & $< 5.0$      & $< 6.0$      & $< 7.0$     \\
\hline
no                     & 92.8\%, 6.4M   & 92.8\%, 2.9M   & 92.7\%, 2.8M & 92.7\%, 2.6M & 92.6\%, 2.1M \\
yes                    & 96.3\%, 5.5M   & 96.3\%, 1.2M   & 96.3\%, 0.8M & 96.2\%, 0.2M & 96.2\%, 0.2M \\
\end{tabular}
\caption{Result of applying different pruning strategies on Turku Dependency Treebank models.}\label{tab:pruning-tdt}
\end{center}
\end{table}


\paragraph{Results} The results for FTB and TDT are shown in Tables
\ref{tab:pruning-ftb} and \ref{tab:pruning-tdt}, respectively. The
results are visualized in Figure \ref{fig:pruning-vis}.

Clearly, mass based pruning is more effective than update count based
pruning. For FTB, without a morphological analyzer, the full accuracy
of $93.2\%$ can be maintained even when pruning out 81\% of model
parameters. When using update count as pruning criterion, full
accuracy cannot be maintained when pruning out more than 38\% of model
parameters. For TDT, the corresponding figures are 55\% for mass based
pruning and 23\% for update based pruning.

When using a morphological analyzer, even further feature pruning is
possible. For FTB, 84\% of model parameters can be pruned while
maintaining full accuracy when using mass based pruning. When using
update count based pruning, however, no parameters can be pruned
without losing accuracy. For TDT, update count based pruning can prune
out 72\% of the features when using a morphological analyzer but mass
based pruning can prune out even more -- 81\%.

\begin{figure}[htb!]
\begin{center}
\includegraphics[width=\textwidth]{pruning}
\end{center}
\caption{This figure visualizes the trade-off between model accuracy
  and model size which can be achieved using {\textcolor{red}{value
      based pruning}} (the red data points) and
  {\textcolor{blue}{update count based pruning}} (the blue data
  points). The black data point in each graph represents the original
  model without pruning. Data points that lie close to the upper
  left corner of the graph represent models that are pruned
  efficiently while maintaining a lot of the original accuracy. In
  contrast, data points closer to the lower right corner represent
  models where pruning is unable to reduce model size effectively but
  the accuracy of the model still degrades. The general tendency is
  that red data points are closer to the upper left corner than blue
  ones, which means that value based pruning is more effective than
  update count based pruning.}\label{fig:pruning-vis}
\end{figure}

\paragraph{Discussion} The guiding principle for pruning based on
update count is that parameters which receive few updates activate
rarely. Thus they are not very influential for tagging
accuracy. Whereas this may give a sufficient criterion for determining
that a parameter is non-influential, it does not give a necessary
condition. There are features that activate often but do not help in
tagging. For example, features sharing the template {\bf The word
  begins with ``a''} activate often in any realistic data set for
Finnish morphological tagging. However, they are almost completely
uninformative. Therefore, provided sufficient data, their update count
will be high but the absolute value of the features will close to zero
because the updates cancel out as the features activate approximately
equally often for all labels. Value based pruning will prune out both
features that activate rarely and features that activate often but do
not provide additional information for the tagging task. It is,
therefore, not surprising that the experiments quite clearly show that
value based pruning is superior on the FTB and TDT data sets.

Models can be pruned more heavily when the morphological analyzer is
used. This probably reflects the fact that the tagging task is easier
when the tagger can rely on the analyzer. For example, a substantial
part of word forms only receive one label from the morphological
analyzer. Therefore, the disambiguation task becomes trivial for these
words.

Value based pruning may not be able to filter out features that are
highly correlated with other features. This case should be handled
using for example L1 regularization for perceptron taggers
\cite{Zhang2014}. 

Training and development times are not included in Tables
\ref{tab:pruning-ftb} and \ref{tab:pruning-tdt}. However, model
development is substantially more time consuming when update count
based pruning is used. This happens because a new model has to be
trained for every threshold, as the threshold influences the model
during training. In contrast, mass based pruning can be performed on a
trained model using several thresholds.

Finally, it is interesting to note that some pruned models yields
better results for TDT than the original models. It may be that
pruning can help to regularize the model in some cases, however, the
differences in these experiments are not statistically significant.

%\section{Lemmatizer}

%Remember to check how often the correct edit script exists.

%Test leaving out the word form as a feature.