\chapter{Conditional Random Fields}

\section{Discriminative modeling}

As seen in Chapter \ref{chapter:hmm}, the HMM POS tagger can be viewed
as a state machine which alternates between sampling words from state
specific observation distributions and sampling morphological labels
from state specific transition distributions. Each emission word and each
morphological label is conditioned {\it solely} on the current
morphological label. These independence assumptions are harsh. For example
collocations cannot be adequately modeled, because the model does not
include direct information about neighboring words in a sentence.

Although information about word sequences and orthography is quite
useful in morphological labeling, it is often difficult to incorporate
such information in a generative model. As \cite{Sutton2012} note, two
principal approaches could, however, be attempted:
\begin{enumerate}
\item Extending the emission model presented in Chapter
  \ref{chapter:hmm} to incorporate dependencies between words and
  orthographic features.\label{ap:1}
\item Replacing the usual emission model with a Naive
  Bayes' model which in theory can handle arbitrary
  features (although overlapping features cause ).\label{ap:2}
\end{enumerate}

Approach \ref{ap:1} is difficult in a fully generative setting because the
emission model needs to account for the complex dependencies that
exist between sentence contexts and orthography. There simply does not
seem to exist a straightforward way of modeling the dependencies.

In a domain closely related to morphological labeling, namely
biomedical entity extraction,\cite{Ruokolainen2013} show that approach
\ref{ap:2} also fails. In fact their experiments indicate that adding richer
context modeling such as adjacent words may worsen the performance of
a tagger with a Naive Bayes emission model. One reason for this may be
that overlapping information sources required in a rich emission model
tend to cause the Naive Bayes model to give overly confident
probability estimates \citep{Sutton2012}. Combining the probabilities
given by the emission model with the transition model can therefore be
problematic.

In contrast to generative sequence models, discriminative sequence
models such as Maximum Entropy Markov Models \citep{Ratnaparkhi1998}
and Conditional Random Fields \citep{Lafferty2001} can incorporate
overlapping sources of information. They model the conditional
distribution of label sequences $p(y\cond x)$ directly instead of
modeling the joint distribution $p(x,\ y)$. Therefore, they do not
need to model the distribution of sentences at all. 

Discriminative models assign probabilities $p(y\cond x)$ for label
sequences $y = ({\rm DT},\ {\rm NN},\ {\rm VBZ},\ {\rm .})$ and word sequences $x = ({\rm The},\ {\rm dog},\ {\rm eats},\ {\rm .})$ by extracting local features from
the input sentence and label sequence. Examples of local features
include {\it ($x_1$ is ``dog'' and $y_1$ is ``NN'')} and {\it ($y_{0}$ is ``DT'' and
  $y_1$ is ``NN'')}. Each feature is associated with a parameter value and the
parameter values are combined to give the conditional likelihood of
the entire label sequence. Naturally, the label sequence which
maximizes the conditional likelihood given sentence $x$ is the label
sequence returned by the discriminative POS tagger.

In generative models, emissions and transitions are independent. Both
are determined exclusively based on the current label. Contrastingly,
in discriminative models, there are no emissions or
transitions. Instead, it is customary to speak about unstructured
features relating exclusively to the input sentence, and structured
features, which incorporate information about the label
sequence. Simplifying a bit, discriminative models make no
independence assumptions among features relating to a single position
in the sentence. This allows for improved fit to training data but
parameter estimation becomes more complex as we shall soon
see. Moreover, discriminative models are more prone to
over-fitting. This is of course an example of the famous bias-variance
trade-off \citep{Geman1992}.


\section{Maximum Entropy Modeling}
\label{sec:me}

\subsection{Example}
\label{sec:maxent-ex}

\section{Basics}
\label{crf:basics}

I will now describe a CRF POS tagger from a practical
point-of-view. The tagging procedure encompasses two stages: feature
extraction and inference using an exact or approximate inference
algorithm. Whereas inference in CRFs is very similar to inference in
HMMs, we did not discuss feature extraction in association to
HMMs. This is because HMM taggers use a restricted set of features
(the current word and preceding labels).

\begin{tabular}{lcccc}
$y$ & DT & NN & VBZ & .\\
$x$ & The & dog & eats & .
\end{tabular}

Features are true logical propositions, which connect aspects of the
input sentence with labels or parts of the label sequence. Given
sentence $x$ and label sequence $y$ of length $n$, we extract features
at every position $t$ in the sentence. For example at position $2$ in
sentence $x$, we could extract {\it The current word $x_t$ is ``dog''
  and the label is ``NN''} and {\it The previous label is ``DT'' and
  the current label is ``NN''}. We could, however, not extract the
same feature {\it The current word is ``dog'' and the label is ``NN''}
at position $1$, because this proposition is false when $t = 1$ (the
word at position 1 is ``The'' and the label is ``VBZ'').

Features are conjunctions of two parts: a feature template, for
example {\it The current word is ``dog''} and a label {\it the
  label is ``NN''}. The set of features recognized by a CRF POS tagger
contains all conjunctions $f \& y$ of feature templates $f$
present in the training data and labels $y$. For example, the tagger
would know {\it The current word is ``dog'' and the label is ``DT''}
although it is unlikely that this feature would ever be observed in
actual training data.

\cite{Ratnaparkhi1996} introduced a rather rudimentary feature set and
variations of this feature set are commonly used in the literature
(for example \cite{Collins2002} and \cite{Lafferty2001}). Let $W$ be
the set of word forms in the training data. Additionally let $P$ and
$S$ be the sets of prefixes and suffixes of maximal length $4$ of all
words $w \in W$. Then, the Ratnaparkhi feature set contains the
unstructured feature templates in Table \ref{tab:uratna} and the
structured feature templates in Table \ref{tab:sratna}.

\begin{table}[!htb]
\begin{tabular}{ll}  
Feature template & Example\\
\hline
The current word is $w$ & The current word is ``dog''\\
The current word has prefix $p$ & The current word has prefix ``d-''\\
The current word has suffix $s$ & The current word has suffix ``-og''\\
The current word contains a digit & \\
The current word contains a hyphen & \\
The current word contains a upper case letter & \\
The previous word is $w$ & The previous word is ``The''. \\
The next word is $w$ & The next word is ``eats''. \\
The word before the previous word is $w$ & \\
The word after the next word is $w$ & \\
\end{tabular}
\caption{foo}\label{tab:uratna}
\end{table}

\begin{table}[!htb]
\begin{tabular}{ll}
Feature template & Example\\
\hline
The label of the previous word is $y$ & The label of the previous word is ``NN''. \\
The label of the previous two words are $y'$ and $y$ & The labels of the two previous words are\\
 & ``DT'' and ``NN''. 
\end{tabular}
\caption{foo}\label{tab:sratna}
\end{table}

These feature templates are then combined with all labels occurring in
the training set. It is instructive to try to estimate the number of
features when using a realistic training set of size around a million
words. The number of features is be $|Y|^3 + |WY|$. For small
label sets and large training data, the bulk of the feature set tends
to consist of unstructured features. However, for large label sets in
the order of $1,000$ labels, there will be a significant number of
structured features (one billion in this case). This necessitates
either dropping second order structured features or using sparse
feature representations. All structured features simply cannot be
represented in memory. We will see techniques to circumvent these
problems. Especially the averaged perceptron is essential.

It is common to represent the CRF using linear algebra. Each position
$t$ in the sentence is represented as a vector $\phi_t$ whose
dimensions correspond to the entire space of possible features. The
selection of features is finite because it is limited by the training
data. There are only finitely many word forms, suffixes, morphological
labels and so on in the training data. The elements of each vector
$\phi_t$ represent activations of features. In the present work all
elements are either $0$ or $1$ mirroring false and true truth values,
but other activations in $\mathbb{R}$ can also be used if the truth
values of the feature propositions exist on a non-binary scale.

In order to represent sentence positions as vectors, we need an
injective index function $I$ which maps features onto consecutive
integers starting at 1. For each feature $f$, $I(f)$ will correspond
to one dimension in $\phi_t$. In concrete implementations, the index
function $I$ can be implemented as a hash function.
 
Given a sentence $x$ and label sequence $y$, we can extract the set of features
$F_t(x)$ for each position $t$ in $x$. Let $\phi_t \in \mathbb{R}^N$ be a vector defined by
$$\phi_t(i) = 1,\ {\rm\ if}\ i \le N\ {\rm and}\ I(f) = i\ {\rm for\ some\ }f\in F_t$$
all other entries in $\phi_t$ are $0$. 

Given a parameter vector $\omega \in \mathbb{R}^N$, the probability $p(y|x)$ is
$$p(y|x) \propto \prod_{t = 1}^T \exp(\omega^\top \phi_t)$$
Specifically, the same parameter vector $\omega$ is shared by all
sentence positions and the probability $p(y|x)$ is a log linear
combination of parameter values in $\omega$.

\cite{Lafferty2001}
\section{Logistic Regression}
%\begin{itemize}
%\item Detailed view of logistic regression. 
%\item Relation to maximum entropy.
%\item Why are MEMMs not better than HMMs?
%\item Label and observation bias.
%\end{itemize}

The simplest Conditional Random Field is the {\it Logistic Regression
  Model} (LRM). It is an unstructured probabilistic discriminative
model. In this section, I will present a formal treatment of the
LRM because it aids in understanding more general
CRFs.

Regular linear regression models a {\it real valued quantity} $y$
based on independent variables $x_1$, ..., $x_n$. In contrast the LRM
is a regression model which models {\it the probability} that an
observation $x$ belongs to a class $y$ in a finite class set $Y$. For
example, the logistic classifier can be used to model the probability
of a tumor belonging to the class {\sc MALIGNANT} or BENIGN. The
probability is based on quantifiable information about the tumor such
as its size, shape and the degrees of expression of different genes in
the tumor cells. These quantifiable information sources are the {\it
  feature templates} of the logistic classifier and combined with
class labels they make up the features of the
model. %In concrete terms, let $f$ be the feature template {\it Tumor diameter greater then 5 cm} and $y$ be the class MALIGNANT, then we can form a feature $f \& y$ ``{\it Tumor diameter greater than 5 cm} and the tumor is MALIGNANT''.

The material at hand deals with linguistic data where most information
sources are binary, for example whether a word ends in suffix ``-s''
and whether a word is preceded by the word ``an''. In other domains
such as medical diagnostics, more general features can be used. These
can be real valued numerical measurements such as the diameter of a
tumor. This treatment of logistic classifiers will focus on the binary
valued case. When using binary features, we can equate the example $x$
with the set of feature templates $F_x \subset F$ that it {\it
  activates}, that is {\it Tumor diameter $\geq$ 5 cm}, {\it Preceded
  by ``an''} and so on. Examples that activate the exactly same
feature templates will be indistinguishable from the point of view of the
Logistic Regression model.

The logistic classifier associates each combination of a feature
template and class with a unique feature and a corresponding real
valued parameter. Intuitively, the logistic classifier models
correlations of feature templates and classes by changing the
parameter values of the associated features. For example, it might
associate the feature template {\it Tumor diameter $\geq$ 5 cm} more
strongly with the class MALIGNANT than the class BENIGN if large
tumors are cancerous more often than smaller ones. This could be
reflected in the parameter values of the model that correspond to the
features $f =$ {\it Tumor diameter $\geq$ 5 cm and class is MALIGNANT}
and $f' =$ {\it Tumor diameter $\geq$ 5 cm and class is BENIGN} so
that the parameter value for $f$ is greater than the parameter value
for $f'$. In general parameter values, however, also depend on other
features and feature correlations in the model. Therefore we can say
that the parameter value of, $f$ will be guaranteed to be greater than
the parameter value of $f'$ when $f$ is the sole feature template and
the model accurately reflects the original distribution of class
labels among examples. In the general case, where there are several
feature templates, this might fail to hold.

Formalizing the notation used in Section \ref{crf:basics}, let $F$ be
a finite set of feature templates and $Y$ a finite set of
classes. Each combination of feature template $f \in F$ and class $y
\in Y$ corresponds to a unique feature. Therefore, the model will have
$|F \times Y|$ features in total. Let $\theta$ be a real valued
parameter vector in $\R^{|F \times Y|}$ and let ${\mathrm I}$ be a
1-to-1 index function which maps each combination of feature template
and class onto the indexes of $\theta$, that is $1 \leq {\mathrm I}(f,
y) \leq |F \times Y|$.

For each example $x$, let $F_x$ be the set of feature templates that $x$ activates and let $y \in Y$ be a class. Then the feature vector associated with $x$ and $y$ is $\phi(x,y) = \{0, 1\}^{|F \times Y|}$ defined by
\[
  \phi(x,y)[i] = \left\{
  \begin{array}{ll}
  1 & {\rm iff\ } i = {\mathrm I}(f,y) {\rm\ for\ some\ }f \in F_x{\rm,} \\ 
  0 & {\rm otherwise.}  
  \end{array}
  \right.
\]

%$$F_y(I(f, y)) = 1\ {\rm iff\ }f \& y\ {\rm is\ true}.$$

Now the conditional probability $p(y\cond x)$ defining the Logistic
classifier is given by Equation \eqref{eq:logistic}. The equation
defines a probability distribution over the set of classes $Y$ because
each quantity $p(y\cond x\parcond\theta)$ is a positive real and the
quantities sum to 1. 

\begin{equation}
p(y\cond x\parcond \theta) = \frac{\exp(\theta^\top \phi(x,y))}{\sum_{z \in Y}\exp(\theta^\top \phi(x,z))}\label{eq:logistic}
\end{equation}

\paragraph{Inference} Inference for a Logistic Regression Model means finding the
probability of each class label $y \in Y$ given example $x$. The full
computation of the probability is, however, not needed when the model
is used as a classifier. For simply finding the class $y_{max}$ which
maximizes the conditional likelihood in equation
\ref{eq:logistic,inference} given fixed parameters $\theta$ it is
sufficient to maximize the numerator of $p(y \cond x\parcond\theta)$.

\begin{equation}y_{max} = \argmax_{y \in Y} p(y\cond x\parcond \theta) = \argmax_{y\in Y} \frac{\exp(\theta^\top \phi(x,y))}{\Z(x;\theta)} = \argmax_{y \in Y} \exp(\theta^\top \phi(x,y))\label{eq:logistic,inference}\end{equation}

To avoid underflow when using finite precision real numbers (such as
floating-point numbers), the maximization is usually rephrased as the
minimization of a cost-function in Equation
\ref{eq:logistic,inference,log}

\begin{equation}y_{max} = \argmin_{y\in Y} \theta^\top \phi(x,y)\label{eq:logistic,inference,log}\end{equation}

From a practical implementation perspective, the minimization in
Equation \ref{eq:logistic,inference,log} boils down to computing one
inner product $\theta^\top \phi(x,y)$ for each label $y \in Y$ and
finding the minimum. Using a suitable sparse approach each of the
inner products can be computed in $\O(|F_x|)$ time, where $F_x$ is the
set of feature templates activated by example $x$. Therefore, the
worst-case complexity of classification is dependent on the size of
the label set $Y$ and the number of feature templates $f \in F$, that
is the complexity is $\O(|Y||F|)$.

\subsection{Estimation}
The Logistic Regression Model is log-linear as demonstrated by
Equation \ref{eq:logistic,log}, which represents the model using a
cost function $\mathcal{L}$.

\begin{equation}
\mathcal{L}(\theta\parcond\mathcal{D}) = -\log p(y\cond x\parcond \theta) = log(\Z(x;\theta)) - \theta^\top F_y(x)\label{eq:logistic,log}
\end{equation}
Here $Z(x;\theta) = \sum_{z \in Y}\exp(\theta^\top F_{z}(x))$ is the partition function of the data point $(x,y)$.

Given labeled training data $\data = \{(x_1,\ y_1),\ ...,\
(x_n,\ y_n)\}$, there exist several options for estimating the
parameters $\theta$. The most commonly used is the maximum likelihood,
or equivalently minimum cost, estimation. The minimum cost estimate
for the parameters $\theta$ using $\data$ is given by equation
\eqref{eq:logistic,ml}.

\begin{equation}
\theta = \argmin_{\theta'} \mathcal{L}(\theta\parcond \mathcal{D}) = \argmin_{\theta'}\sum_{(x,\ y) \in \mathcal{D}} \Z(x\parcond \theta) - {\theta'}^\top F_y(x)\label{eq:logistic,ml}
\end{equation}

The probability $p(y\cond x\parcond \theta)$ has exponential form,
which means that the probability is proportional to a product of
factors of the form $e^{ap}$, where $a$ is an activation (0 or 1) and
$p$ is a parameter. This has three important consequences:

\begin{enumerate}
\item The function $\theta \mapsto p(y\cond x\parcond \theta)$ is smooth.
\item The function $\theta \mapsto p(y\cond x\parcond \theta)$ is convex.
\item There exists a {\it unique} $\theta$ maximizing the likelihood of the training data $\mathcal{D}$.\footnote{Technically this requires that the possible values of $\theta$ are limited into a compact subset of the parameter space.}
\item The model $p(y\cond x\parcond \theta)$ is maximally unbiased. 
\end{enumerate}

Smoothness follows from the fact that each factor $a \mapsto e^{ap}$
is smooth and products and sums of smooth functions are
smooth. Convexity of the likelihood follows by a straightforward application of the Hölder inequality \cite{}. Property 3 is a consequence of properties 1 and 2 and
Property 4 follows from the discussion in Section \ref{sec:me}.

Although the maximization in Equation \ref{eq:logistic} cannot be
solved exactly in general, the convexity and smoothness of
$p(y\cond x\parcond \theta)$ mean that efficient numerical methods can
be used for approximating the maximum.

Gradient based methods such as SGD (leading to online estimation) and
L-BFGS (leading to batch estimatiom) require information about the
partial derivatives of the cost function. Therefore the partial
derivatives $\partial\mathcal{L}(\theta\parcond
\mathcal{D})/\partial\i$ need to be computed. Examining Equation
\ref{eq:logistic,ml}, we can see that the cost consists of two terms
$f(\theta\parcond \mathcal{D}) = \log(\Z(\mathcal{D}\parcond \theta))$
and $g(\theta\parcond\mathcal{D}) = \sum_{(x,y) \in
  \mathcal{D}}\theta^\top F_y(x)$. The partial derivative of $g$
w.r.t. parameter $i$ can be computed in the following way
$$\frac{\partial g}{\partial i} = \sum_{(x,y) \in \mathcal{D}} F_y(x)[i]$$
This quantity represents the total activation of feature $i$ in the training data and is called {\it the observed count} of feature $i$. Using the chain rule of derivatives, we get the partial derivative of $f$ w.r.t. to param $i$, is
$$\frac{\partial f}{\partial i} = \sum_{(x,y) \in \mathcal{D}} \frac{\sum_{y \in \mathcal{Y}}F_y(x)[i] \exp(\theta^\top F_y(x))}{\Z(x\parcond\theta)} = \sum_{(x,y) \in \mathcal{D}} \sum_{y' \in \mathcal{Y}}F_y(x)[i] p(y'|x\parcond\theta)$$
This is the {\it expected count} of feature $i$ which is the
activation of feature $i$ in the data set $x_1, ...,x_n$ predicted by
the model given all possible label assignments.

Using the partial derivatives of the functions $f$ and $g$, we see that the gradient of the cost function $\mathcal{L}$ is defined by
\begin{equation}\nabla L[i] = \sum_{(x,y) \in \mathcal{D}} \Big( \sum_{y' \in \mathcal{Y}}F_y(x)[i] p(y'|x\parcond\theta) \Big) - F_y(x)[i] \label{eq:reg-cost}\end{equation}
Equation \ref{eq:reg-cost} shows that the cost is zero when the
expected and observed counts for each feature agree. The properties
for the logistic regression model discussed above guarantee that this
there is at most one $\theta_{ML}$ like this and, when it exists,
$\theta_{ML}$ is the maximum likelihood estimate for the parameters.

Regularization methods such as $L_1$ and $L_2$ introduced in Chapte
\ref{chap:ml} can also be applied to the model. This naturally changes
the gradient and also the properties of the model. Analysis of the
regularized model falls outside of the scope of this thesis.

\section{The Perceptron Classifier}

The perceptron algorithm \citep{Rosenblatt1958} is an alternative to the MLE
for learning the weights of a discriminative classifier. As seen
above, the logistic classifier optimizes the conditional probability
of gold standard classes given training inputs. In contrast, the
perceptron rule directly optimizes the classification performance of
the discriminative classifier.

Intuitively, the multi-class perceptron algorithm works by labeling
each training example in order using a current estimate of the
parameter vector $\theta$ and adjusting the parameter vector whenever
training examples are incorectly labeled. Consequently, the perceptron
algorithm is an online learning algorithm.

\paragraph{Inference} Similarly as in the case of any linear
classifier, each example $x$ and class $y$ receives a score
$\theta^\top\phi(x,y)$ which is computed using the current estimate of
the parameter vector. If the score of the gold standard class
$y_{gold}$ is not the highest one, that is there is a class $y$ which
scores higher or equally high, then the parameter vector $\theta$ is
modified in a way which increases the score for the gold standard
class and decreases the score for the highest scoring class $y$.

\paragraph{Estimation} The perceptron algorithm is an error-driven
online learning algorithm. When a classification error is encountered
during estimation, that is $\theta^\top\phi(x,y) >
\theta^\top\phi(x,y_{gold})$, the parameter vector $\theta$ is
adjusted for relevant features. For every feature template $f$ which
is activated by the example $x$, the weights $\theta[I(f, y_{gold})]$
and $\theta[I(f, y)]$ are adjusted. Here $I(f, y_{gold})$ and $I(f,
y)$ are the features corresponding to the template $f$ and classes
$y_{gold}$ and $y$ repectively. The perceptron rule for weight
adjustment is the following:
$$\theta[I(f, y_{gold})] = \theta[I(f, y_{gold})] + 1 {\rm\ and\ }\theta[I(f, y)] = \theta[I(f, y)] - 1$$

\begin{algorithm}[!p]
\begin{center}
\caption{The pass of the perceptron algorithm in Python 3.}\label{forward-algorithm}
\begin{lstlisting}[linewidth=\textwidth]
def infer(x, fextractor, theta, label_set)
    """
        x          - An obesrvation.
        fextractor - A vector valued function. 
                     len(fextractor(x,y)) == len(theta). 
        theta      - A parameter vector.
        label_set  - Set of potential labels. 
    """
    sys_label = None
    max_score = -float('inf')
 
    for y in label_set:
        score = dot_product(theta, fextractor(x,y))
        if score > max_score:
            max_score = score
            sys_label = label

    assert(sys_label != None)
    return sys_label

def perceptron(data, fextractor, theta, label_set): 
    """
        data       - data[i][0] is an observation, data[i][1] a label.
        fextractor - A vector valued function. 
                     len(fextractor(x,y)) == len(theta) 
        theta      - The parameter vector.
        label_set  - Set of potential labels. 

        Run one pass of the perceptron algorithm.
    """

    for x, y_gold in data:
         y_system = infer(x, fextractor, theta, label_set)
         
         if y_system != y_gold:
             for f in fextractor(x, y_system):
                 theta[f] -= 1
             for f in fextractor(x, y_gold):
                 theta[f] += 1
\end{lstlisting}
\end{center}
\end{algorithm}

The perceptron adjustment does not guarantee that example $x$ is
correctly classified. However, it does guarantee that the score
difference between the gold class and erroneous class decreases\footnote{There are refiniements of the perceptron algorithm, such as the passive-aggressive learning algorithm, which aim to make fewer updates by updating more aggressively when the difference in scores between the erroneous class and gold class is large \citep{Crammer2006}}. Given
training data consisting of just one example, it is easy to see that
the perceptron algorithm will eventually classify the example
correctly. If there are more examples, it may however happen that a
correct parameter vector is never found.

The perceptron algorithm {\it converges} when no example in the
training data causes a change in the parameter vector
$\theta$. Equivalently, the perceptron algorithm correctly classifies
every example in the training data. It can be showed that the
perceptron algorithm converges whenever there exists a parameter vector
that correctly classifies the training data \citep{Freund1999}. Such a
data set is called linearly separable. The term originates from a
geometrical interpretation of the 2-class perceptron algorithm, where
the parameter vector defines a hyper plane in the feature space which
divides the apce into two halves. A data set is called separable if
there is a hyper space which separates the examples in each of the
classes into their own half space.

When a data set is linearly separable, there are typically infinitely
many parameter vectors that that classify the data set correctly. The
perceptron algorithm will give one of these. Other algorithms exist
which attempt to find an optimal parameter vector in some sense
(e.g. Support Vector Machines \citep{Cortes1995}). These, however,
fall beyond the scope of the present work.

Even when the training set is not linearly separable, the perceptron
algorithm will have good performance in practice. In the non-separable
case, a held out development set is used. Training is stopped when the
performance of the classifier on the development set no longer
improves.

\paragraph{Voting and Averaging} Because the perceptron algorithm
makes fixed updates of size $1$, the parameter vector tends to change
too rapidly at the end of the training procedure. To avoid this, it is
customary to use the average of all parameter vectors from the
training procedure instead of the final parameter vector. This will
give better performance during test time. Parameter averaging is an
approximation of so called {\it voting perceptron}. In voting, each
parameter vector is considered a separate classifier and the
classification is performed by taking a majority vote of all of the
classification results. This is impractical, because there are
thousands of classifiers. Therefore, averaging is used in order to
achieve almost the same effect.

\section{CRF -- A Structured Logistic Classifier}

This Section presents Linear Chain Conditional Random Fields
(CRF)\footnote{More general CRF models can be formulated but these
  mostly fall beyond the scope of this thesis. See \citep{Sutton2012}
  for further details.}. Just as the HMM is a structured equivalent of
the NB classifier, the CRF is the structured equivalent of the
LRM. Consequently, many of the algorithms required to build a CRF
tagger are similar to the algorithms required to build an HMM
tagger. Estimation of model parameters is, however, different because
of the discriminative nature of the model.

Another major difference between an HMM classifier and a CRF
classifier is that the CRF classifier typically employs a much larger
set of features. This increases the size of the model. It also makes
the discriminative tagger slow in comparison to the generative
tagger. The slowdown is demonstrated by the experiments in
\cite{Silfverberg2015}. However, the accuracy of the discriminative
model is significantly superior to the generative HMM tagger.

Intuitively, the CRF model resembles a sequence of LR classifiers
with shared parameters. Given a sentence $x = (x_1,\ ...,\ x_T)$,
label sequence $y = (y_1,\ ...,\ y_T)$ and parameters $\theta$ for the
LR model, a score for the label $y_i$ in position $i$ $s(x, y_{i-2},
y_{i-1}, y_i, i\parcond \theta)$ can be computed. Here the LR model
utilizes the input sentence $x$ as well as labels $y_{i - 2}$, $y_{i-1}$ and
$y_i$ to extract unstructured and structured features from the
sentence and label sequence. The score $s$ takes on a familiar form
$$s(x, y_{i-2}, y_{i-1}, y_i, i\parcond \theta) = \theta^\top F_y(x, i, y_{i-2}, y_{i-1})$$
where $F_y$ is a vector valued feature extraction function. Each
feature associated to the label $y$ corresponds to one element of the
vector $F_y(x, i, y_{i-2}, y_{i-1})$. As in the case of the LR model,
each entry of the vector can be an arbitrary real number but in this
thesis they will always be either $0$ or $1$.

The probability of label sequence $y \in \mathcal{Y}^T$ given a sentence $x$ of length $T$ is
\begin{equation}\label{eq:crf}p(y|x\parcond\theta) \propto \prod_{i = 1}^T s(x, y_{i-2}, y_{i-1}, y_i, i\parcond \theta)\end{equation}
In Equation \ref{eq:crf}, the labels $y_{-1}$ and $y_0$ are special
stop labels which do not belong to the label set $\mathcal{Y}$. The partition function of the sentence $x$ is given by
\begin{equation}\sum_{y\in \mathcal{Y}^T}\prod_{i = 1}^T s(x, y_{i-2}, y_{i-1}, y_i, i\parcond \theta)\end{equation}

It is noteworthy, that the probability in Equation \ref{eq:crf} is
normalized for {\it the entire sentence}, not in each position. A
similar model, where normalization happens in each position is called
the Maximum Entropy Markov Model (MEMM). It has been shown to give
inferior performance in POS tagging of English
\citep{Lafferty2001}\footnote{The inferior performance of the MEMM has
  been thought to be a result of the so called label bias problem
  \citep{Lafferty2001} although {\it observation} bias may be more
  influential in POS tagging \citep{Klein2002}.}.

\paragraph{Inference} Tagging of a sentence using the CRF model is
very similar to HMM tagging. The major difference is that there are
far more features in a CRF model which slows down inference compared
to a typical HMM tagger. As in the case of an HMM, the Viterbi
algorithm has to be used to find the MAP assignment of the label
sequence because of the structured nature of the model. The
forward-backward algorithm can be used to compute the marginal
probabilities of labels.

\paragraph{Estimation} Estimation of the CRF model parameters is more
involved than the straightforward counting which is sufficient for HMM
training. Estimation is instead very similar to estimation of the LR
model parameters. However, the structured nature of the CRF model
complicates matters slightly.

Let $\mathcal{D} = \{(x_1, y_1), ..., (x_n, y_n)\}$ be a training data
set consisting of $n$ labeled sentences and let $y_k^l$ be the label
of the $l$th word in sentence $l$. Now the expected count of feature
$i$ in position $k$ in sentence $l$ is
$$\frac{\sum_{y \in \mathcal{Y}^T}F_y(x_l, k, y_{k - 2}, y_{k-1}, y_k)[i] \exp(\theta^\top F_y(x))}{\Z(x\parcond\theta)}$$
The quantities $\sum_{y \in \mathcal{Y}^T}F_y(x_l, k, y_{k - 2},
y_{k-1}, y_k)[i]$ and $\Z(x\parcond\theta)$ have to be computed using
the forward backward algorithm because the number of computations is
too large otherwise. The need of the forward backward algorithm is the
most important difference between LR and CRF estimation.

Commonly, the SGD algorithm and L-BFGS are used for the optimization
of $\theta$ \citep{Vishwanathan2006}.

\begin{itemize}
\item The term perceptron tagger is commonly found in the litterature, e.g. \cite{Collins2002}.
\item The model is called a CRF.
\item The estimator can be a ML, Perceptron, pseudo likelihood, pseudo
  perceptron...
\item Dev data for adjusting hyper parameters: number of iterations and
  regularization hyper-parameters, model order.
\item Hierarchical CRF \cite{Muller2013}, \cite{Weiss2010} and \cite{Charniak2005}.
\end{itemize}

\section{The Perceptron Tagger}

Whereas the perceptron classifier is a discriminative classifier
similar to the LR model except that it uses perceptron estimation, a
perceptron tagger \citep{Collins2002} is a sequence labeling model
similar to the CRF except that it uses perceptron estimation.

The model is formulated very similarly as the CRF model. It also uses
a real valued parameter vector $\theta$ and the score of a label
sequence $y$ given sentence $x$ is defined as
\begin{equation}s(y|x\parcond\theta) \propto \prod_{i = 1}^T s(x,
  y_{i-2}, y_{i-1}, y_i, i\parcond
  \theta)\label{eq:perc-classifier}\end{equation} In Equation
\ref{eq:perc-classifier}, $s(x, y_{i-2}, y_{i-1}, y_i, i\parcond
\theta) = \theta^\top F_y(x, i, y_{i-2}, y_{i-1})$, where $F_y$ is the
vector valued feature extraction function for label $y$.

\paragraph{Inference} The Perceptron tagger uses the Viterbi algorithm
for exact inference. Beam search can be used for faster approximate
inference together with a label dictionary \citep{Silfverberg2015}.

\paragraph{Estimation}Whereas, CRF estimation requires the
forward-backward algorithm, perceptron estimation only requires the
Viterbi algorithm.  Exactly as in the case of the regular perceptron
algorithm, each training example (i.e. sentence) is labeled and
unstructured and structured features are updated accordingly. The
number of training epochs is determined using held-out data. Parameter
averaging is useful for improving the accuracy of the percetron
tagger.

\paragraph{Beam search for estimation} A high model order and large
label set can result in a prohibitive runtime for the Viterbi
algorithm which has a complexity that is dependent on the $n$th power
of the label set size for an order $n$ model. This problem can be
avoided using beam search during estimation instead of the Viterbi
algorithm. Because beam search is an approximative inference
algorithm, it may however not give the correct MAP assignment for a
sentence. It may happen that beam search returns a label sequence
$y_{sys}$ for training sentence $x$ whose score w.r.t. to model
parameters is lower than the score of the gold label sequence
$y_{gold}$. This lead to perceptron updates which are not necessary
because the model already correctly labels sentence $x$ if only exact
inference is used. [WHY IS THIS SO BAD?]

\paragraph{Violation fixing} To avoid superfluous perceptron updates,
a technique called violation fixing can be used \citep{Huang2012}
(this is an extension of the early update technique suggested for
incremental parsing in \cite{Collins2004}). \cite{Huang2012} suggest
several related violation fixing methods. The essence of the methods
is to compute the score each prefix $y_{sys}^i$ of the label sequence
$y_{sys}$ returned by beam search and $y_{gold}^i$ the gold standard
label sequence $y_{gold}$. One $i$ is then selected so that the score
of $y_{sys}^i > y_{gold}^i$ (if such an $i$ exists). Updates are then
performed for $y_{sys}^i$ and $y_{gold}^i$. The choice of $i$ depends
on the violation fixing method. For example, the last $i$ which
exhibits a violation can be used.

In the experiments presented in \cite{Silfverberg2015}, violation
fixing did not result in consistent statistically significant
improvements. It may be that it is more influential in parsing than
POS tagging or morphological tagging.

\paragraph{Label guessing} As in the case of the CRF tagger, a
cascaded model can be used to shorten training time
\citep{Silfverberg2015}. Instead of a cascade of discriminative
classifiers, used in \citep{Muller2013}, \cite{Silfverberg2015} use a
combination of a generative label guesser of the type presented in
Section \ref{sec:hmm-counting}. The number of guesses can be
determined either beased on a probability mass threshold ot using a
fixed number of guesses per word. Setting the threshold too low will
result in a training task that is to easy. Consequently the model will
overfit the training data. Higher thresholds will approximate the
original training task more cloasely but will also lead to longer
training times.

Like beam search, label guessing modifies the training task. It does,
however, not require violation fixing because it does not influence
the relative difference in scores of label sequences as long as the
gold standard label sequence is never pruned out. Therefore, the gold
standard label should always be added to the set of guesses given by
the label guesser.

The combination of beam search and model cascading results in a fast
training with a tolerable decrease in tagging accuracy even large
label sets and for second order models as shown in Section
\ref{sec:finnpos}.

\section{Enriching the Structured Model}\label{sec:sub-labels}
\begin{itemize}
\item Utilizing sub-label structure.
\end{itemize}

\section{Model Pruning}\label{sec:pruning}

Discriminative models for morhpological tagging can often grow quite
large in terms of parameter count. For example, the model learned from
the FTB corpus by the FinnPos tagger has more than 4 million
parameters.

A large number of parameters is problematic because it causes
over-fitting of the model to the training data. Moreover, large models
can be problematic when memory foot-print is an issue: e.g. on mobile
devices.

Different methods have been propsed for pruning of perceptron
models. \cite{Goldberg2011} prune the models based on update
count. Parameters that receive less than a fixed amount of updates
during training will be omitted from the final model. Another approach
is to pruning by feature count. For example \cite{Hulden2013} prune
out features for words occurring less than a fixed amount of times in
the training data. More generally, fetures that are activate less than
a fixed amount of times may be pruned out. 

Some regularization techiques can also be used to learn sparse
perceptron models. L1-regularization yields sparse models similarly as
for logistic regression. \cite{Zhang2014} investigate
L1-regularization for structured perceptron. They gain accuracy but do
not report results on model size.

I have explored two different pruning strategies
\begin{itemize}
\item Pruning based on update counts \citep{Goldberg2011}.
\item Pruning based on parameter value.
\end{itemize}
\cite{Goldberg2011} show that update count based pruning beats feature
count based pruning in dependency parsing and POS tagging. Therefore,
I decided not to compare those approaches. Instead I compare update
count based pruning to pruning based on final parameter value.

\paragraph{Update Count Pruning} When using this strategy, each
parameter which did not receive at least $n$ updates during training,
is omitted from the final model. Here $n$ is a hyper-parameter which
is set using held-out data. In practice, this pruning strategy
requires that one maintains a update count vector where each element
corresponds to one model parameter. Whenever a parameter is updated
during training, the update count is increased.

As stated before, the perceptron algorithm labels a training example
and then performs updates on the model parameters. When labeling
during training, only those parameters that already received at least
$n$ updates are used. However, updates are performed on all
parameters. When the update count of a parameter exceeds $n$, the
parameter value will therefore already be of similar magnitude with
the rest of the parameter values in the model.

\citep{Goldberg2011} do not explore early stopping. Preliminary
experiments showed that it is best to first set the number of training
passes without parameter pruning and then set the pruning threshold
$n$ separately using develpment data. If the number of passes and the
update count threshold are set at the same time, the model parameters
converge quite slowly resulting in many training epochs and
consequently many parameter updates. This has an adverse effect on the
number of parameters that can be pruned from the final model.

\paragraph{Value Based Pruning} A very simple strategy for
parameter pruning is to prune based on the parameter value. The model
is trained in the regular manner. After training, all parameters whose
absolute value does not exceed a threshold $\kappa$ are omitted from
the model. Remaining parameter values remain unchanged. The
hyper-parameter $\kappa$ is determined using a development set.

In the experiment chapter, I show that value based pruning outperforms
update count based pruning on the data-sets that I have used. In some
settings, the difference is substantial.
